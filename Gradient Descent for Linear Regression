## Gradient Descent for Linear Regression

### 1. Problem Setup
We are trying to fit a linear model \( h_\theta(X) = X\theta \) to the data.

- \( X \) is the matrix of input features (of size \( m \times n \)), where \( m \) is the number of training examples, and \( n \) is the number of features.
- \( y \) is the vector of target values (of size \( m \times 1 \)).
- \( \theta \) is the vector of parameters we want to learn (of size \( n \times 1 \)).
- \( \alpha \) is the learning rate, which controls how big a step we take during each iteration of gradient descent.
- \( iterations \) is the number of times we perform the gradient descent step.

### 2. Hypothesis Function
The hypothesis function for linear regression is:

\[
h_\theta(X) = X\theta
\]

Here, \( h_\theta(X) \) is a vector of predictions.

### 3. Cost Function
The cost function (or the objective function) for linear regression is given by the Mean Squared Error (MSE):

\[
J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]

Alternatively, in vectorized form:

\[
J(\theta) = \frac{1}{2m} (X\theta - y)^T (X\theta - y)
\]

### 4. Gradient of the Cost Function
To minimize \( J(\theta) \), we compute its gradient with respect to \( \theta \):

First, let's differentiate \( J(\theta) \) with respect to \( \theta \):

\[
\frac{\partial J(\theta)}{\partial \theta} = \frac{\partial}{\partial \theta} \left( \frac{1}{2m} (X\theta - y)^T (X\theta - y) \right)
\]

Using the chain rule, the derivative of \( J(\theta) \) with respect to \( \theta \) is:

\[
\nabla_\theta J(\theta) = \frac{1}{m} X^T (X\theta - y)
\]

This gradient vector \( \nabla_\theta J(\theta) \) gives the direction and magnitude by which we should update \( \theta \) to minimize the cost function.

### 5. Gradient Descent Update Rule
Gradient descent updates \( \theta \) iteratively using the following rule:

\[
\theta := \theta - \alpha \nabla_\theta J(\theta)
\]

Substituting the gradient from the previous step:

\[
\theta := \theta - \alpha \cdot \frac{1}{m} X^T (X\theta - y)
\]

Here, \( \alpha \) controls the learning rate, i.e., how big a step we take in the direction of the gradient.

### 6. Implementing the Update Rule
In the provided code:

- **Initialization:** \( \theta \) is initialized as a zero vector of size \( n \times 1 \):

\[
\theta = \text{np.zeros}((n, 1))
\]

- **Predictions:** In each iteration, predictions are computed as:

\[
\text{predictions} = X @ \theta = X\theta
\]

- **Errors:** The error between predictions and actual values is calculated:

\[
\text{errors} = \text{predictions} - y
\]

- **Gradient Calculation:** The gradient is computed as:

\[
\text{updates} = \frac{1}{m} X^T \text{errors}
\]

- **Parameter Update:** The parameters \( \theta \) are updated using:

\[
\theta := \theta - \alpha \cdot \text{updates}
\]

### 7. Convergence
This process is repeated for a specified number of iterations. Over time, the parameters \( \theta \) converge to values that minimize the cost function \( J(\theta) \).

### 8. Final Output
After the loop completes, the learned parameters \( \theta \) are returned, rounded to four decimal places for precision:

\[
\text{np.round}(\theta.flatten(), 4)
\]

### 9. Python Implementation

```python
import numpy as np

def linear_regression_gradient_descent(X: np.ndarray, y: np.ndarray, alpha: float, iterations: int) -> np.ndarray:
    m, n = X.shape
    theta = np.zeros(n)
    for i in range(iterations):
        y_hat = X @ theta
        dJ_dtheta = X.T @ (y_hat - y) / m
        theta -= alpha * dJ_dtheta
    return theta
